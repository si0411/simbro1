#!/usr/bin/env python3
"""
Scrape all group tour URLs and extract data into group_tours_frontend.json
Reads URLs from group_tour_urls.json (generated by get_grouptour_urls.py)
"""

from categorized_extractor import CategorizedTourExtractor
import json
import time
import os
import tempfile
import shutil
from datetime import datetime

def log_error(message):
    """Log errors to a dedicated error log file"""
    try:
        error_log = 'scraping_errors.log'
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(error_log, 'a') as f:
            f.write(f"[{timestamp}] {message}\n")
    except Exception as e:
        print(f"âš ï¸ Could not write to error log: {e}")

def atomic_write_json(data, filename):
    """
    Write JSON data to file atomically to prevent corruption
    Returns (success: bool, error_message: str)
    """
    try:
        # Get the directory of the target file
        target_dir = os.path.dirname(os.path.abspath(filename)) if os.path.dirname(filename) else '.'

        # Create a temporary file in the same directory
        fd, temp_path = tempfile.mkstemp(suffix='.json', dir=target_dir, text=True)

        try:
            # Write JSON data to temp file
            with os.fdopen(fd, 'w') as f:
                json.dump(data, f, indent=2)

            # Atomically replace the old file with the new one
            shutil.move(temp_path, filename)

            # Set permissions to be readable by web server
            os.chmod(filename, 0o644)

            print(f"âœ… Successfully wrote: {filename}")
            return True, None

        except Exception as e:
            # Clean up temp file if write failed
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise e

    except Exception as e:
        error_msg = f"Failed to write {filename}: {str(e)}"
        print(f"âŒ {error_msg}")
        log_error(error_msg)
        return False, error_msg

def safe_remove_file(filepath):
    """
    Safely remove a file with error handling
    Returns (success: bool, error_message: str)
    """
    try:
        if os.path.exists(filepath):
            os.remove(filepath)
            print(f"ğŸ—‘ï¸ Removed: {filepath}")
            return True, None
        else:
            return True, "File does not exist"
    except Exception as e:
        error_msg = f"Could not remove {filepath}: {str(e)}"
        print(f"âš ï¸ {error_msg}")
        log_error(error_msg)
        return False, error_msg

def scrape_all_grouptours():
    """Scrape all group tours from the JSON URL list"""
    
    # Load the URLs from JSON file
    try:
        with open('group_tour_urls.json', 'r') as f:
            url_data = json.load(f)
        
        # Extract just the bt_url values
        urls = [item['bt_url'] for item in url_data if item.get('bt_url')]
        
    except FileNotFoundError:
        print("âŒ Error: group_tour_urls.json not found!")
        print("   Run get_grouptour_urls.py first to discover URLs")
        return []
    except Exception as e:
        print(f"âŒ Error loading URLs from JSON: {e}")
        return []
    
    print(f"ğŸš€ Processing {len(urls)} tour URLs...")
    print("=" * 70)
    
    extractor = CategorizedTourExtractor()
    all_tour_data = []
    
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] {url.split('/')[-1]}")
        
        try:
            tour_data = extractor.extract_tour_data(url)
            all_tour_data.append(tour_data)
            
            # Show progress
            tour_name = tour_data.get('tour_name', 'Unknown')
            dates_count = len(tour_data.get('starting_dates', []))
            prices_count = sum(1 for v in tour_data.get('price', {}).values() if v)
            
            print(f"  âœ… {tour_name}")
            print(f"     ğŸ“… {dates_count} dates | ğŸ’° {prices_count} currencies")
            
        except Exception as e:
            print(f"  âŒ Error: {e}")
            all_tour_data.append({'url': url, 'error': str(e)})
        
        # Add delay between requests to be respectful
        time.sleep(2)
        
        # Save progress every 10 tours
        if i % 10 == 0:
            print(f"\nğŸ’¾ Saving progress after {i} tours...")
            with open(f'group_tours_progress_{i}.json', 'w') as f:
                json.dump(all_tour_data, f, indent=2)
    
    # Save final results with timestamp
    import glob

    print(f"\nğŸ’¾ Saving final results...")

    # Add timestamp to the data and remove individual tour timestamps to avoid conflicts
    timestamp = datetime.now().strftime('%Y-%m-%d')

    # Clean the tour data by removing individual last_updated fields to avoid confusion
    cleaned_tour_data = []
    for tour in all_tour_data:
        if isinstance(tour, dict) and 'error' not in tour:
            # Create a copy and remove any old last_updated field from individual tours
            clean_tour = {k: v for k, v in tour.items() if k != 'last_updated'}
            cleaned_tour_data.append(clean_tour)
        else:
            cleaned_tour_data.append(tour)

    final_data = {
        'last_updated': timestamp,
        'tours': cleaned_tour_data
    }

    # Save with timestamp in filename for versioning
    timestamped_filename = f'group_tours_frontend_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'

    # Write main file using atomic write
    main_success, main_error = atomic_write_json(final_data, 'group_tours_frontend.json')
    if not main_success:
        error_msg = f"CRITICAL: Failed to save main file: {main_error}"
        print(f"âŒ {error_msg}")
        log_error(error_msg)
        # Continue anyway to try timestamped backup

    # Write timestamped backup using atomic write
    backup_success, backup_error = atomic_write_json(final_data, timestamped_filename)
    if not backup_success:
        error_msg = f"WARNING: Failed to save backup file: {backup_error}"
        print(f"âš ï¸ {error_msg}")
        log_error(error_msg)

    # Only report success if at least one file was written
    if main_success or backup_success:
        if main_success:
            print(f"ğŸ“ Saved main file: group_tours_frontend.json")
        if backup_success:
            print(f"ğŸ“ Saved timestamped file: {timestamped_filename}")
    else:
        error_msg = "CRITICAL: Failed to save both main and backup files!"
        print(f"âŒ {error_msg}")
        log_error(error_msg)
        return all_tour_data  # Return early if all writes failed

    # Cleanup operations after successful scraping
    print(f"\nğŸ§¹ Cleaning up old files...")

    # Remove progress files from this run
    progress_files = glob.glob('group_tours_progress_*.json')
    for progress_file in progress_files:
        safe_remove_file(progress_file)

    # Remove backup files older than 7 days
    current_time = time.time()
    for backup_file in glob.glob('group_tours_frontend_backup_*.json'):
        try:
            file_age = current_time - os.path.getmtime(backup_file)
            if file_age > 7 * 24 * 3600:  # 7 days in seconds
                safe_remove_file(backup_file)
        except Exception as e:
            print(f"âš ï¸ Could not check age of {backup_file}: {e}")

    # Remove timestamped files older than 3 days (keep some for backup)
    for timestamped_file in glob.glob('group_tours_frontend_2*.json'):
        if timestamped_file != timestamped_filename:  # Don't remove the one we just created
            try:
                file_age = current_time - os.path.getmtime(timestamped_file)
                if file_age > 3 * 24 * 3600:  # 3 days in seconds
                    safe_remove_file(timestamped_file)
            except Exception as e:
                print(f"âš ï¸ Could not check age of {timestamped_file}: {e}")

    print(f"âœ… Cleanup completed")
    
    # Summary
    successful = len([t for t in all_tour_data if 'error' not in t])
    failed = len(all_tour_data) - successful
    
    print(f"\nğŸ‰ EXTRACTION COMPLETE!")
    print(f"   âœ… Successfully processed: {successful} tours")
    print(f"   âŒ Failed: {failed} tours") 
    print(f"   ğŸ“ Results saved to: group_tours_frontend.json")
    
    # Show sample of tour names extracted
    print(f"\nğŸ·ï¸  Sample tour names extracted:")
    for i, tour in enumerate(all_tour_data[:5], 1):
        if 'error' not in tour:
            name = tour.get('tour_name', 'Unknown')
            print(f"   {i}. {name}")
    
    return all_tour_data

if __name__ == "__main__":
    scrape_all_grouptours()